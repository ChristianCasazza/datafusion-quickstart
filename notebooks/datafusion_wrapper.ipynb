{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFusionWrapper Class- open the box to see the underlying code\n",
    "from pathlib import Path\n",
    "from datafusion import SessionContext\n",
    "\n",
    "class DataFusionWrapper:\n",
    "    def __init__(self):\n",
    "        self.con = SessionContext()\n",
    "        self.registered_tables = []\n",
    "\n",
    "    def register_data(self, paths, table_names):\n",
    "        \"\"\"\n",
    "        Registers data files (Parquet, CSV, JSON) to DataFusion context with specified table names.\n",
    "        Automatically detects the file type based on the file extension.\n",
    "        \n",
    "        Args:\n",
    "            paths (list): List of paths to data files.\n",
    "            table_names (list): List of table names corresponding to the paths.\n",
    "        \"\"\"\n",
    "        if len(paths) != len(table_names):\n",
    "            raise ValueError(\"The number of paths must match the number of table names.\")\n",
    "        \n",
    "        for path, table_name in zip(paths, table_names):\n",
    "            file_extension = Path(path).suffix.lower()\n",
    "\n",
    "            if file_extension == \".parquet\":\n",
    "                self.con.register_parquet(table_name, path)\n",
    "            elif file_extension == \".csv\":\n",
    "                self.con.register_csv(table_name, path)\n",
    "            elif file_extension == \".json\":\n",
    "                self.con.register_json(table_name, path)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type '{file_extension}' for file: {path}\")\n",
    "\n",
    "            self.registered_tables.append(table_name)\n",
    "\n",
    "    def run_query(self, sql_query):\n",
    "        \"\"\"\n",
    "        Runs a SQL query on the registered tables in the DataFusion context.\n",
    "        Args:\n",
    "            sql_query (str): The SQL query string to execute.\n",
    "        Returns:\n",
    "            DataFrame: Query result as a DataFusion DataFrame.\n",
    "        \"\"\"\n",
    "        return self.con.sql(sql_query)\n",
    "\n",
    "    def _construct_path(self, path, base_path, file_name, extension):\n",
    "        \"\"\"\n",
    "        Constructs the full file path based on input parameters.\n",
    "        \"\"\"\n",
    "        if path:\n",
    "            return Path(path)\n",
    "        elif base_path and file_name:\n",
    "            return Path(base_path) / f\"{file_name}.{extension}\"\n",
    "        else:\n",
    "            # Default file path: \"output.<extension>\" in the current directory\n",
    "            return Path(f\"output.{extension}\")\n",
    "\n",
    "    def export(self, dataframe, file_type, path=None, base_path=None, file_name=None, with_header=True):\n",
    "        \"\"\"\n",
    "        Exports a DataFrame to the specified file type.\n",
    "        Args:\n",
    "            dataframe: DataFusion DataFrame.\n",
    "            file_type (str): Type of file to export ('parquet', 'csv', 'json').\n",
    "            path: Full path to the file (optional).\n",
    "            base_path: Directory path (optional).\n",
    "            file_name: Name of the file (without extension) (optional).\n",
    "            with_header: Include header row for CSV files (default: True).\n",
    "        \"\"\"\n",
    "        file_type = file_type.lower()\n",
    "        if file_type not in [\"parquet\", \"csv\", \"json\"]:\n",
    "            raise ValueError(\"file_type must be one of 'parquet', 'csv', or 'json'.\")\n",
    "\n",
    "        # Construct file path\n",
    "        full_path = self._construct_path(path, base_path, file_name, file_type)\n",
    "\n",
    "        # Export based on file type\n",
    "        if file_type == \"csv\":\n",
    "            dataframe.write_csv(full_path, with_header=with_header)\n",
    "        elif file_type == \"json\":\n",
    "            dataframe.write_json(full_path)\n",
    "        elif file_type == \"parquet\":\n",
    "            dataframe.write_parquet(full_path)\n",
    "        \n",
    "        print(f\"File written to: {full_path}\")\n",
    "\n",
    "    def show_tables(self):\n",
    "        \"\"\"\n",
    "        Displays the table names and types currently registered in the catalog.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, table_type \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema <> 'information_schema'\n",
    "        \"\"\"\n",
    "        result_df = self.run_query(query)\n",
    "        result_df.show()\n",
    "\n",
    "    def show_schema(self, table_name):\n",
    "        \"\"\"\n",
    "        Displays the schema of the specified table.\n",
    "        \n",
    "        Args:\n",
    "            table_name (str): Name of the table whose schema is to be displayed.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            table_name, \n",
    "            column_name, \n",
    "            data_type\n",
    "        FROM \n",
    "            information_schema.columns \n",
    "        WHERE \n",
    "            table_name = '{table_name}'\n",
    "        \"\"\"\n",
    "        result_df = self.run_query(query)\n",
    "        result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the wrapper\n",
    "con = DataFusionWrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load local files into DataFusion\n",
    "# Dynamically find the root of the repo (assuming the .ipynb is located within the repo)\n",
    "repo_root = Path.cwd().resolve().parents[0]   # Adjust to locate the repo root\n",
    "\n",
    "# Define relative paths from the repo root and corresponding table names\n",
    "paths = [\n",
    "    repo_root / \"data/examples/mta_operations_statement/file_1.parquet\",\n",
    "    repo_root / \"data/examples/mta_hourly_subway_socrata/*.parquet\",\n",
    "    repo_root / \"data/examples/mta_daily_ridership/*.parquet\",\n",
    "    repo_root / \"data/examples/mta_bus_wait_time/*.parquet\",\n",
    "    repo_root / \"data/examples/daily_weather_asset/*.parquet\",\n",
    "    repo_root / \"data/examples/csv_example/*.csv\",\n",
    "    repo_root / \"data/examples/json_example/*.json\",\n",
    "]\n",
    "table_names = [\n",
    "    \"mta_operations_statement\",\n",
    "    \"mta_hourly_subway_socrata\",\n",
    "    \"mta_daily_ridership\",\n",
    "    \"mta_bus_wait_time\",\n",
    "    \"daily_weather_asset\",\n",
    "    \"operations_csv\",\n",
    "    \"operations_json\"\n",
    "]\n",
    "\n",
    "# Register data files (automatically detects file type)\n",
    "con.register_data(paths, table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame()\n",
      "+---------------------------+------------+\n",
      "| table_name                | table_type |\n",
      "+---------------------------+------------+\n",
      "| mta_daily_ridership       | BASE TABLE |\n",
      "| operations_json           | BASE TABLE |\n",
      "| mta_operations_statement  | BASE TABLE |\n",
      "| daily_weather_asset       | BASE TABLE |\n",
      "| mta_bus_wait_time         | BASE TABLE |\n",
      "| operations_csv            | BASE TABLE |\n",
      "| mta_hourly_subway_socrata | BASE TABLE |\n",
      "+---------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# Quickly see all of the tables that are loaded and their names\n",
    "con.show_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame()\n",
      "+--------------------------+---------------------+-----------+\n",
      "| table_name               | column_name         | data_type |\n",
      "+--------------------------+---------------------+-----------+\n",
      "| mta_operations_statement | fiscal_year         | Int64     |\n",
      "| mta_operations_statement | timestamp           | Date32    |\n",
      "| mta_operations_statement | scenario            | LargeUtf8 |\n",
      "| mta_operations_statement | financial_plan_year | Int64     |\n",
      "| mta_operations_statement | expense_type        | LargeUtf8 |\n",
      "| mta_operations_statement | agency              | LargeUtf8 |\n",
      "| mta_operations_statement | type                | LargeUtf8 |\n",
      "| mta_operations_statement | subtype             | LargeUtf8 |\n",
      "| mta_operations_statement | general_ledger      | LargeUtf8 |\n",
      "| mta_operations_statement | amount              | Float64   |\n",
      "| mta_operations_statement | agency_full_name    | LargeUtf8 |\n",
      "+--------------------------+---------------------+-----------+\n"
     ]
    }
   ],
   "source": [
    "#Show the schema of a table\n",
    "con.show_schema(\"mta_operations_statement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame()\n",
      "+------------+\n",
      "| total_rows |\n",
      "+------------+\n",
      "| 67763465   |\n",
      "+------------+\n"
     ]
    }
   ],
   "source": [
    "# Write a simple SQL query against the tables and print result as a dataframe\n",
    "sql_query = \"\"\"\n",
    "select count(*) as total_rows \n",
    "from mta_hourly_subway_socrata\n",
    "\"\"\"\n",
    "\n",
    "# Run query and show results\n",
    "result_df = con.run_query(sql_query)\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written to: /home/christianocean/datafusion-quickstart/data/exports/row_count.csv\n"
     ]
    }
   ],
   "source": [
    "# Export Dataframes into Parquet, CSV, or JSON\n",
    "file_type = \"csv\"\n",
    "file_name = \"row_count\"\n",
    "base_path = repo_root / \"data/exports\"\n",
    "\n",
    "# Export results of result_df\n",
    "con.export(result_df, file_type=file_type, base_path=base_path, file_name=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame()\n",
      "+----------------------------------------------------------------------+--------------------+----------------------+-------------------+---------------------+--------------------+----------------+\n",
      "| general_ledger                                                       | first_payment_date | first_payment_amount | last_payment_date | last_payment_amount | average_payment    | total_payments |\n",
      "+----------------------------------------------------------------------+--------------------+----------------------+-------------------+---------------------+--------------------+----------------+\n",
      "| 2 Broadway Certificates of Participation (2 Bdwy COPS w/o Int + Int) | 2019-01-01         | 0.0                  | 2023-12-01        | 434020.78           | 115023.14730392158 | 60             |\n",
      "| Debt Service for Payroll Mobility Tax Bonds                          | 2021-01-01         | 0.0                  | 2024-06-01        | 34215833.25         | 5759055.434345238  | 42             |\n",
      "| Net Dedicated Tax Fund Bonds Debt Service                            | 2019-01-01         | 4566.66              | 2024-06-01        | 91244560.33         | 15161793.468257576 | 66             |\n",
      "| Net Transportation Revenue Bonds Debt Service                        | 2019-01-01         | 0.0                  | 2024-06-01        | 139872962.27        | 28591719.355900005 | 66             |\n",
      "| TBTA General Revenue Bonds                                           | 2019-01-01         | 2670023.43           | 2024-06-01        | 48088819.55         | 16266166.344797982 | 66             |\n",
      "| TBTA Subordinate Revenue Bonds                                       | 2019-01-01         | 0.0                  | 2024-06-01        | 7255885.56          | 2481823.3714646464 | 66             |\n",
      "|                                                                      |                    |                      | 2024-06-01        |                     | 17855.092555555555 | 30             |\n",
      "+----------------------------------------------------------------------+--------------------+----------------------+-------------------+---------------------+--------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# Write a more complciated SQL query\n",
    "sql_query = \"\"\"\n",
    "WITH payment_stats AS (\n",
    "    SELECT\n",
    "        general_ledger,\n",
    "        MIN(timestamp) AS first_payment_date,\n",
    "        MAX(timestamp) AS last_payment_date,\n",
    "        AVG(amount) AS average_payment,\n",
    "        COUNT(DISTINCT timestamp) AS total_payments\n",
    "    FROM\n",
    "        mta_operations_statement\n",
    "    WHERE\n",
    "        scenario = 'Actual'\n",
    "        AND type = 'Debt Service Expenses'\n",
    "    GROUP BY\n",
    "        general_ledger\n",
    "),\n",
    "first_payment AS (\n",
    "    SELECT\n",
    "        general_ledger,\n",
    "        MIN(timestamp) AS first_payment_date,\n",
    "        MIN(amount) AS first_payment_amount\n",
    "    FROM\n",
    "        mta_operations_statement\n",
    "    WHERE\n",
    "        scenario = 'Actual'\n",
    "        AND type = 'Debt Service Expenses'\n",
    "    GROUP BY\n",
    "        general_ledger\n",
    "),\n",
    "last_payment AS (\n",
    "    SELECT\n",
    "        general_ledger,\n",
    "        MAX(timestamp) AS last_payment_date,\n",
    "        MAX(amount) AS last_payment_amount\n",
    "    FROM\n",
    "        mta_operations_statement\n",
    "    WHERE\n",
    "        scenario = 'Actual'\n",
    "        AND type = 'Debt Service Expenses'\n",
    "    GROUP BY\n",
    "        general_ledger\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    ps.general_ledger,\n",
    "    fp.first_payment_date,\n",
    "    fp.first_payment_amount,\n",
    "    ps.last_payment_date,\n",
    "    lp.last_payment_amount,\n",
    "    ps.average_payment,\n",
    "    ps.total_payments\n",
    "FROM\n",
    "    payment_stats ps\n",
    "LEFT JOIN\n",
    "    first_payment fp ON ps.general_ledger = fp.general_ledger\n",
    "LEFT JOIN\n",
    "    last_payment lp ON ps.general_ledger = lp.general_ledger\n",
    "ORDER BY\n",
    "    ps.general_ledger;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Run query and show results\n",
    "result_df = con.run_query(sql_query)\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame()\n",
      "+-----------------+---------------------+------------------------+-----------+-----------+------------------------+----------------------------+\n",
      "| station_complex | week_start          | total_weekly_ridership | latitude  | longitude | avg_weekly_temperature | total_weekly_precipitation |\n",
      "+-----------------+---------------------+------------------------+-----------+-----------+------------------------+----------------------------+\n",
      "| 1 Av (L)        | 2022-01-31T00:00:00 | 44875                  | 40.730953 | -73.98163 | 26.785714285714285     | 36.9                       |\n",
      "| 1 Av (L)        | 2022-02-07T00:00:00 | 93646                  | 40.730953 | -73.98163 | 37.357142857142854     | 14.7                       |\n",
      "| 1 Av (L)        | 2022-02-14T00:00:00 | 99510                  | 40.730953 | -73.98163 | 32.51428571428571      | 15.200000000000001         |\n",
      "| 1 Av (L)        | 2022-02-21T00:00:00 | 61907                  | 40.730953 | -73.98163 | 38.27142857142858      | 25.2                       |\n",
      "| 1 Av (L)        | 2022-02-28T00:00:00 | 100928                 | 40.730953 | -73.98163 | 37.042857142857144     | 5.800000000000001          |\n",
      "| 1 Av (L)        | 2022-03-07T00:00:00 | 102571                 | 40.730953 | -73.98163 | 40.0                   | 34.6                       |\n",
      "| 1 Av (L)        | 2022-03-14T00:00:00 | 100996                 | 40.730953 | -73.98163 | 49.31428571428571      | 10.1                       |\n",
      "| 1 Av (L)        | 2022-03-21T00:00:00 | 105281                 | 40.730953 | -73.98163 | 45.25714285714286      | 38.3                       |\n",
      "| 1 Av (L)        | 2022-03-28T00:00:00 | 105851                 | 40.730953 | -73.98163 | 40.614285714285714     | 10.5                       |\n",
      "| 1 Av (L)        | 2022-04-04T00:00:00 | 109828                 | 40.730953 | -73.98163 | 47.24285714285714      | 99.49999999999999          |\n",
      "| 1 Av (L)        | 2022-04-11T00:00:00 | 96213                  | 40.730953 | -73.98163 | 54.65714285714285      | 6.5                        |\n",
      "| 1 Av (L)        | 2022-04-18T00:00:00 | 95968                  | 40.730953 | -73.98163 | 48.857142857142854     | 34.9                       |\n",
      "| 1 Av (L)        | 2022-04-25T00:00:00 | 79107                  | 40.730953 | -73.98163 | 50.42857142857143      | 5.8999999999999995         |\n",
      "| 1 Av (L)        | 2022-05-02T00:00:00 | 101024                 | 40.730953 | -73.98163 | 53.31428571428571      | 60.8                       |\n",
      "| 1 Av (L)        | 2022-05-09T00:00:00 | 107395                 | 40.730953 | -73.98163 | 59.300000000000004     | 9.8                        |\n",
      "| 1 Av (L)        | 2022-05-16T00:00:00 | 112689                 | 40.730953 | -73.98163 | 66.12857142857143      | 27.900000000000002         |\n",
      "| 1 Av (L)        | 2022-05-23T00:00:00 | 102669                 | 40.730953 | -73.98163 | 63.88571428571429      | 19.2                       |\n",
      "| 1 Av (L)        | 2022-05-30T00:00:00 | 100894                 | 40.730953 | -73.98163 | 69.68571428571428      | 18.5                       |\n",
      "| 1 Av (L)        | 2022-06-06T00:00:00 | 108977                 | 40.730953 | -73.98163 | 69.17142857142856      | 19.800000000000004         |\n",
      "| 1 Av (L)        | 2022-06-13T00:00:00 | 111919                 | 40.730953 | -73.98163 | 70.1                   | 17.7                       |\n",
      "+-----------------+---------------------+------------------------+-----------+-----------+------------------------+----------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Write a SQL query that includes a join between tables\n",
    "sql_query = \"\"\"\n",
    "WITH weekly_ridership AS (\n",
    "    SELECT \n",
    "        station_complex, \n",
    "        DATE_TRUNC('week', transit_timestamp) AS week_start,\n",
    "        SUM(ridership) AS total_weekly_ridership,\n",
    "        MIN(latitude) AS latitude,  -- Assuming latitude is the same for each station complex, use MIN() or MAX()\n",
    "        MIN(longitude) AS longitude  -- Assuming longitude is the same for each station complex, use MIN() or MAX()\n",
    "    FROM \n",
    "        mta_hourly_subway_socrata\n",
    "    GROUP BY \n",
    "        station_complex, \n",
    "        DATE_TRUNC('week', transit_timestamp)\n",
    "),\n",
    "weekly_weather AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('week', date) AS week_start,\n",
    "        AVG(temperature_mean) AS avg_weekly_temperature,\n",
    "        SUM(precipitation_sum) AS total_weekly_precipitation\n",
    "    FROM \n",
    "        daily_weather_asset\n",
    "    GROUP BY \n",
    "        DATE_TRUNC('week', date)\n",
    ")\n",
    "SELECT \n",
    "    wr.station_complex, \n",
    "    wr.week_start, \n",
    "    wr.total_weekly_ridership,\n",
    "    wr.latitude,\n",
    "    wr.longitude,\n",
    "    ww.avg_weekly_temperature,\n",
    "    ww.total_weekly_precipitation\n",
    "FROM \n",
    "    weekly_ridership wr\n",
    "LEFT JOIN \n",
    "    weekly_weather ww\n",
    "ON \n",
    "    wr.week_start = ww.week_start\n",
    "WHERE \n",
    "    wr.week_start < '2024-09-17'\n",
    "ORDER BY \n",
    "    wr.station_complex, \n",
    "    wr.week_start\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Run query and show results\n",
    "result_df = con.run_query(sql_query)\n",
    "result_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
